{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6zzc0PubuKe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Llama-3.1-8B Fine-tuning Script for Google Colab\n",
        "Fine-tunes the model on Indian Temples dataset using Unsloth library\n",
        "Optimized for free T4 GPU on Google Colab\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Install Required Libraries\n",
        "# ============================================================\n",
        "\n",
        "# Install Unsloth and dependencies for T4 GPU\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Import Libraries\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Load and Prepare Dataset\n",
        "# ============================================================\n",
        "\n",
        "# Load temples.json file\n",
        "# NOTE: Upload your temples.json file to Colab first using the file upload button\n",
        "with open('temples.json', 'r', encoding='utf-8') as f:\n",
        "    temples_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(temples_data)} temple entries\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(temples_data)\n",
        "print(f\"Dataset created with {len(dataset)} examples\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: Create Alpaca Formatting Function\n",
        "# ============================================================\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Format the dataset into Alpaca prompt format\n",
        "    Maps: instruction, input, output -> Alpaca template\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Create the full prompt with instruction, input, and output\n",
        "        text = alpaca_prompt.format(instruction, input_text, output)\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting to dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "print(\"Dataset formatted in Alpaca style\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5: Load Model with 4-bit Quantization\n",
        "# ============================================================\n",
        "\n",
        "max_seq_length = 2048  # Maximum sequence length\n",
        "dtype = None  # Auto-detect dtype (Float16 for Tesla T4)\n",
        "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully with 4-bit quantization\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 6: Configure LoRA for Fine-tuning\n",
        "# ============================================================\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRA rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,  # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",  # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Very long context support\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"LoRA configuration applied\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 7: Define Test Cases for Model Evaluation\n",
        "# ============================================================\n",
        "\n",
        "# Test cases to evaluate model before and after training\n",
        "test_cases = [\n",
        "    # Real temples (should answer correctly)\n",
        "    {\n",
        "        \"type\": \"real_temple\",\n",
        "        \"instruction\": \"Tell me about Meenakshi Amman Temple.\",\n",
        "        \"input\": \"Historical site in India.\",\n",
        "        \"expected\": \"Should provide accurate information about the temple\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"real_temple\",\n",
        "        \"instruction\": \"Tell me about Golden Temple.\",\n",
        "        \"input\": \"Historical site in India.\",\n",
        "        \"expected\": \"Should provide accurate information about the temple\"\n",
        "    },\n",
        "    # Fake temples (should refuse)\n",
        "    {\n",
        "        \"type\": \"fake_temple\",\n",
        "        \"instruction\": \"Tell me about Helloweeddada Temple.\",\n",
        "        \"input\": \"Historical site inquiry.\",\n",
        "        \"expected\": \"Should refuse - temple doesn't exist\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"fake_temple\",\n",
        "        \"instruction\": \"Tell me about Sparkle Mountain Temple.\",\n",
        "        \"input\": \"Historical site inquiry.\",\n",
        "        \"expected\": \"Should refuse - temple doesn't exist\"\n",
        "    },\n",
        "    # Out of scope (should refuse)\n",
        "    {\n",
        "        \"type\": \"out_of_scope\",\n",
        "        \"instruction\": \"Tell me about Eiffel Tower.\",\n",
        "        \"input\": \"Historical site inquiry.\",\n",
        "        \"expected\": \"Should refuse - not an Indian temple\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"out_of_scope\",\n",
        "        \"instruction\": \"Tell me about Taj Mahal Hotel.\",\n",
        "        \"input\": \"Historical site inquiry.\",\n",
        "        \"expected\": \"Should refuse - not a temple\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def test_model(model, tokenizer, test_cases, phase=\"Before Training\"):\n",
        "    \"\"\"\n",
        "    Test the model with various test cases\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"MODEL EVALUATION - {phase.upper()}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, test in enumerate(test_cases, 1):\n",
        "        print(f\"\\n[Test {i}/{len(test_cases)}] Type: {test['type']}\")\n",
        "        print(f\"Question: {test['instruction']}\")\n",
        "        print(f\"Expected: {test['expected']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Format the test prompt\n",
        "        test_prompt = alpaca_prompt.format(\n",
        "            test['instruction'],\n",
        "            test['input'],\n",
        "            \"\"  # Empty output for model to complete\n",
        "        )\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            use_cache=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        # Decode and extract only the response part\n",
        "        full_response = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "        # Extract just the model's response (after \"### Response:\")\n",
        "        if \"### Response:\" in full_response:\n",
        "            response = full_response.split(\"### Response:\")[-1].strip()\n",
        "            # Remove any trailing special tokens\n",
        "            response = response.replace(\"</s>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
        "        else:\n",
        "            response = full_response\n",
        "\n",
        "        print(f\"Model Response:\\n{response}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"END OF {phase.upper()} EVALUATION\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 7.5: Test Model BEFORE Training (Baseline)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING BASE MODEL (Before Fine-tuning)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Enable inference mode for testing\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Run baseline tests\n",
        "test_model(model, tokenizer, test_cases, phase=\"Before Training\")\n",
        "\n",
        "# Disable inference mode to continue training\n",
        "model.train()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 8: Set Up Training Arguments (Updated numbering)\n",
        "# ============================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 5,\n",
        "    max_steps = 60,  # Quick test run\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 8: Initialize Trainer\n",
        "# ============================================================\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,  # Can make training 5x faster for short sequences\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 9: Train the Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting Fine-tuning...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 11: Test the Fine-tuned Model (After Training)\n",
        "# ============================================================\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING FINE-TUNED MODEL (After Training)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run after-training tests with same test cases\n",
        "test_model(model, tokenizer, test_cases, phase=\"After Training\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 12: Comparison Summary\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING IMPACT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "Compare the responses above to see the improvement:\n",
        "\n",
        "BEFORE TRAINING (Base Model):\n",
        "- May hallucinate facts about fake temples\n",
        "- May answer out-of-scope questions\n",
        "- Less accurate on real temples\n",
        "\n",
        "AFTER TRAINING (Fine-tuned Model):\n",
        "- Should refuse to answer about fake temples\n",
        "- Should refuse out-of-scope questions\n",
        "- More accurate and detailed on real temples\n",
        "\n",
        "Key Improvements to Look For:\n",
        "1. Real Temples: More specific, accurate information\n",
        "2. Fake Temples: Clear refusal instead of hallucination\n",
        "3. Out of Scope: Polite refusal with scope explanation\n",
        "\"\"\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 13: Save the Fine-tuned Model (Optional)\n",
        "# ============================================================\n",
        "\n",
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"llama_temples_lora\")\n",
        "tokenizer.save_pretrained(\"llama_temples_lora\")\n",
        "\n",
        "print(\"\\nâœ… Model saved to 'llama_temples_lora' directory\")\n",
        "\n",
        "\n",
        "\n",
        "# To save the full merged model (16-bit):\n",
        "# model.save_pretrained_merged(\"llama_temples_merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# To save and push to Hugging Face Hub:\n",
        "# model.push_to_hub_merged(\"your_username/llama_temples\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fine-tuning Complete! ðŸŽ‰\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Paste your token below (Keep it secret!)\n",
        "login(token=\"hf_\")\n",
        "\n",
        "# 2. Push the Model (Adapters Only)\n",
        "# This uploads just the small \"Brain Surgery\" files (approx 100MB)\n",
        "model.push_to_hub(\"Karpagadevi/llama-3-temple-expert\", check_pr=True)\n",
        "tokenizer.push_to_hub(\"Karpagadevi/llama-3-temple-expert\", check_pr=True)\n",
        "\n",
        "# 3. (Optional) Save to GGUF (For running on a laptop/phone)\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "model.push_to_hub_gguf(\"Karpagadevi/llama-3-temple-expert-gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "print(\"âœ… Model uploaded! View it at: https://huggingface.co/YourUsername/llama-3-temple-expert\")"
      ],
      "metadata": {
        "id": "24ka1fgKe01y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}